# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed up
D_xy
D_yx
I_xy
totalStateMat
totalStateMat <- matrix(data = c(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1), ncol = 3, byrow = T)
totalStateMat
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind) / total
h_x <- p_x * log2(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task) / total
h_y <- p_y * log2(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task] / total
# calculate log portion
p_x <- sum(normMat[ind, ]) / total
p_y <- sum(normMat[ , task]) / total
logVal <- log2(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
# Sum values
H_x <- -sum(H_x)
H_y <- -sum(H_y)
I_xy <- sum(unlist(I_xy))
# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed up
D_xy
D_yux
D_yx
task
ind = 1
task = 1
p_xy <- normMat[ind, task] / total
p_xy
total
normMat[ind, task]
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log2(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log2(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log2(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
# Sum values
H_x <- -sum(H_x)
H_y <- -sum(H_y)
I_xy <- sum(unlist(I_xy))
# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed u
D_xy
H+x
H_x
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log2(p_x)
})
H_x
rowSums(normMat)
normMat
TotalStateMat <- matrix(data = c(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1), ncol = 3, byrow = T)
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log2(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log2(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log2(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
# Sum values
H_x <- -sum(H_x)
H_y <- -sum(H_y)
I_xy <- sum(unlist(I_xy))
# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed up
# Dataframe
D_xu
D_xy
D_yx
rm(list = ls())
source("scripts/__Util__MASTER.R")
source("scripts/3A_PrepPlotExperimentData.R")
load("output/__RData/FixedDelta06Sigma01Eta7100reps.Rdata")
# bind task distributions into mega dataframe
taskDist <- unlist(groups_taskDist, recursive = FALSE)
taskDistTot <- do.call("rbind", taskDist)
taskDistTot[ ,1:2] <- taskDistTot[ ,1:2] * 10000
taskDistTot <- taskDistTot %>%
mutate(Set = paste0(n, "-", replicate)) %>%
arrange(n, replicate)
sets <- unique(taskDistTot$Set)
entropy <- lapply(sets, function(i) {
# Grab task dist dataframe for set
taskDist <- taskDistTot[taskDistTot$Set == i, ]
n <- taskDist$n[1]
replicate <- taskDist$replicate[1]
# Calcualte entropy
taskDist <- taskDist[ ,1:2]
entropy <- mutualEntropy(taskDist)
entropy$n <- n
entropy$replicate <- replicate
return(entropy)
})
entropy
entropy <- do.call("rbind", entropy)  %>%
mutate(set = paste(n, replicate, sep = "-"))%>%
select(-Dsym, -Dyx) %>%
filter(n != 1)
entropy
plot(entropy$n, entropy$Dxy)
mutualEntropy
Hx
TotalStateMat <- matrix(data = c(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1), ncol = 3, byrow = T)
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log2(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log2(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log2(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
# Sum values
H_x <- -sum(H_x)
H_y <- -sum(H_y)
I_xy <- sum(unlist(I_xy))
# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed up
# Dataframe
D_xy
I
I_xy
H_x
H_y
TotalStateMat
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log2(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log2(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log2(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
I_xy
normMat
p_xy
ind = 1
task = 1
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
p_x
p_y
p_xy <- normMat[ind, task]
p_xy / (p_x * p_y)
ind = 2
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log2(p_xy / (p_x * p_y))
logVal
p_xy
p_x
p_y
(p_x * p_y)
p_xy / (p_x * p_y)
log2(p_xy / (p_x * p_y))
ln(2)
rm(list = ls())
source("scripts/__Util__MASTER.R")
source("scripts/3A_PrepPlotExperimentData.R")
load("output/__RData/FixedDelta06Sigma01Eta7100reps.Rdata")
# bind task distributions into mega dataframe
taskDist <- unlist(groups_taskDist, recursive = FALSE)
taskDistTot <- do.call("rbind", taskDist)
taskDistTot[ ,1:2] <- taskDistTot[ ,1:2] * 10000
taskDistTot <- taskDistTot %>%
mutate(Set = paste0(n, "-", replicate)) %>%
arrange(n, replicate)
sets <- unique(taskDistTot$Set)
entropy <- lapply(sets, function(i) {
# Grab task dist dataframe for set
taskDist <- taskDistTot[taskDistTot$Set == i, ]
n <- taskDist$n[1]
replicate <- taskDist$replicate[1]
# Calcualte entropy
taskDist <- taskDist[ ,1:2]
entropy <- mutualEntropy(taskDist)
entropy$n <- n
entropy$replicate <- replicate
return(entropy)
})
entropy
entropy <- do.call("rbind", entropy)  %>%
mutate(set = paste(n, replicate, sep = "-"))%>%
select(-Dsym, -Dyx) %>%
filter(n != 1)
plot(entropy$n, entropy$Dxy)
mutualEntropy()
mutualEntropy
TotalStateMat <- matrix(data = c(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1), ncol = 3, byrow = T)
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log(p_y)
})
ind = 2
task = 1
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
p_x
logVal <- log(p_xy / (p_x * p_y))
normMat <- TotalStateMat / sum(TotalStateMat) #checking this after noticing possible error
# Total Individuals
n <- nrow(normMat)
m <- ncol(normMat)
total <- sum(normMat)
# Shannon's entropy of individuals H(X)
H_x <- apply(normMat, MARGIN = 1, function(ind) {
p_x <- sum(ind)
h_x <- p_x * log(p_x)
})
# Shannon's entropy of tasks H(Y)
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log(p_y)
})
# Mutual entropy I(X,Y)
I_xy <- lapply(1:n, function(ind) {
# Loop through tasks for each individual
mutualEntr <- rep(NA, m)
for (task in 1:m) {
# joint probability p(x,y)
p_xy <- normMat[ind, task]
# calculate log portion
p_x <- sum(normMat[ind, ])
p_y <- sum(normMat[ , task])
logVal <- log(p_xy / (p_x * p_y))
# If entry has zero probability, set total value to zero (instead of NA/-Inf)
entry <- p_xy * logVal
if (is.na(entry)) {
entry <- 0 # setting to zero because if p_x or p_y is 0, then you will get Inf/NA, but times 0 is 0 (approx)
}
# enter into list
mutualEntr[task] <- entry
}
mutualEntr <- sum(mutualEntr)
return(mutualEntr)
})
# Sum values
H_x <- -sum(H_x)
H_y <- -sum(H_y)
I_xy <- sum(unlist(I_xy))
# Calcualte symmetrid division of labor D(x,y)
D_sym <- I_xy / sqrt(H_x * H_y)
D_yx <- I_xy / H_x #names mixed up
D_xy <- I_xy / H_y #names mixed up
# Dataframe
D_xy
Hx
H_x
H_y
normMat
H_y <- apply(normMat, MARGIN = 2, function(task) {
p_y <- sum(task)
h_y <- p_y * log(p_y)
})
H_y
p_y
task = 1
task = normMat[,1]
task
p_y <- sum(task)
p_y
h_y <- p_y * log(p_y)
h_y
log(p_y)
p_y
log(normMat)
normMat*log(normMat)
rm(list = ls())
source("scripts/__Util__MASTER.R")
library(scales)
library(RColorBrewer)
####################
# Prep and Plot
####################
##### Delta 06 #####
# load
load("output/ParameterExploration/Rdata/FixedDelta06_SigmaSlopeExplorationEXTRA.Rdata")
improve1 <- improve %>%
mutate(relativePercInc = (PercIncrease - 1.220554) / 1.220554,
relativeSlope   = (SlopeIncrease - 0.02322321) / 0.02322321,
relativeLarge   = (SpecLarge - 0.5915000) / 0.5915000,
relativeSmall   = (SpecSmall - 0.2663750) / 0.2663750,
Increase        = SlopeIncrease * 14) %>%
mutate(fit = (abs(relativeLarge) + abs(relativeSmall) + abs(relativeSlope)) / 3,
fitRMSE = sqrt( (relativeLarge^2 + relativeSmall^2 + relativeSlope^2) / 3 ))
load("output/ParameterExploration/Rdata/FixedDelta06_SigmaSlopeExplorationEXTRA2.Rdata")
improve2 <- improve %>%
mutate(relativePercInc = (PercIncrease - 1.220554) / 1.220554,
relativeSlope   = (SlopeIncrease - 0.02322321) / 0.02322321,
relativeLarge   = (SpecLarge - 0.5915000) / 0.5915000,
relativeSmall   = (SpecSmall - 0.2663750) / 0.2663750,
Increase        = SlopeIncrease * 14) %>%
mutate(fit = (abs(relativeLarge) + abs(relativeSmall) + abs(relativeSlope)) / 3,
fitRMSE = sqrt( (relativeLarge^2 + relativeSmall^2 + relativeSlope^2) / 3 ))
load("output/ParameterExploration/Rdata/FixedDelta06_SigmaSlopeExploration.Rdata")
improve <- improve %>%
mutate(relativePercInc = (PercIncrease - 1.220554) / 1.220554,
relativeSlope   = (SlopeIncrease - 0.02322321) / 0.02322321,
relativeLarge   = (SpecLarge - 0.5915000) / 0.5915000,
relativeSmall   = (SpecSmall - 0.2663750) / 0.2663750,
Increase        = SlopeIncrease * 14) %>%
mutate(fit = (abs(relativeLarge) + abs(relativeSmall) + abs(relativeSlope)) / 3,
fitRMSE = sqrt( (relativeLarge^2 + relativeSmall^2 + relativeSlope^2) / 3 ))
improve06 <- rbind(improve, improve1, improve2)
rm(improve, improve1, improve2)
# Filter to size
improve06 <- improve06 %>%
filter(!sigma %in% c(0.075, 0.125, 0.175, 0.225, 0.275, 0.325))
View(improve06)
###############################################################################
rm(list = ls())
source("scripts/__Util__MASTER.R")
library(scales)
library(RColorBrewer)
####################
# Prep and Plot
####################
# load
load("output/ParameterExploration/Rdata/FixedDelta06_SigmaSlopeExploration.Rdata")
improve <- improve %>%
mutate(relativePercInc = (PercIncrease - 1.220554) / 1.220554,
relativeSlope   = (SlopeIncrease - 0.02322321) / 0.02322321,
relativeLarge   = (SpecLarge - 0.5915000) / 0.5915000,
relativeSmall   = (SpecSmall - 0.2663750) / 0.2663750,
Increase        = SlopeIncrease * 14) %>%
mutate(fit = (abs(relativeLarge) + abs(relativeSmall) + abs(relativeSlope)) / 3,
fitRMSE = sqrt( (relativeLarge^2 + relativeSmall^2 + relativeSlope^2) / 3 ))
# Set file names
filename <- "PerCapitaWorkload"
View(improve)
source('~/Documents/Research/Tarnita Lab/Incipient Groups DOL/DOLThresholdModel/scripts/other_scripts/3_SinglePlotResultsLargerSizes.R', echo=TRUE)
