# Sample task probability
tasks_order <- order(P_sub_g[row, ])
order(c(1, 1))
stimMat
P_sub_g[row, 1] == P_sub_g[row, 2]
TimeStep = t
TimeStep
# Get relevant stimulus levels
stim_levels <- StimulusMatrix[TimeStep, 1:2]
StimulusMatrix = stimMat
# Get relevant stimulus levels
stim_levels <- StimulusMatrix[TimeStep, 1:2]
stim_levels
tasks_order <- order(stim_levels)
tasks_order
tasks_order <- order(stim_levels, decreasing = T)
tasks_order
source("scripts/__Util__MASTER.R")
####################################################################
rm(list = ls())
source("scripts/__Util__MASTER.R")
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- c(16) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 10000 #number of generations to run simulation
corrStep       <- 200 #number of time steps for calculation of correlation
reps           <- 1 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- c(10, 10) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- c(0, 0) #intital vector of stimuli
StimRates      <- c(0.6, 0.6) #vector of stimuli increase rates
threshSlope    <- 30 #exponent parameter for threshold curve shape
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
####################
# Run simulation multiple times
####################
# Prep meta-lists for collection of group size simulations
groups_taskDist  <- list()
groups_taskCorr  <- list()
groups_taskStep  <- list()
groups_taskTally <- list()
groups_stim      <- list()
groups_entropy   <- list()
# Loop through group sizes
for (i in 1:length(Ns)) {
# Set group size
n <- Ns[i]
# Prep lists for collection of simulation outputs
ens_taskDist  <- list()
ens_taskCorr  <- list()
ens_taskStep  <- list()
ens_taskTally <- list()
ens_entropy   <- list()
ens_stim      <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- initiateProbMatrix(n = n, m = m)
# Seed task (external) stimuli
stimMat <- seedStimuls(InitialSVector = InitialStim,
RateVector = StimRates,
gens = gens)
# Seed internal thresholds
threshMat <- seedThresholds(n = n,
m = m,
ThresholdMeans = ThreshM,
ThresholdSDs = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
taskStep <- list()
taskTally <- list()
####################
# Simulate
####################
# Run simulation
for (t in 1:gens) {
# Update stimuli
for (j in 1:(ncol(stimMat)/2)) {
# update stim
stimMat[t + 1, j] <- globalStimUpdate(stimulus = stimMat[t, j],
delta = stimMat[t, j + m],
alpha = alpha,
Ni = sum(X_g[ , j]),
n = n)
# shift down delta (rate increases)
stimMat[t + 1, j + m] <- stimMat[t, j + m]
}
# Calculate task demand based on global stimuli
P_g <- calcThresholdDetermMat(TimeStep = t + 1, # first row is generation 0
ThresholdMatrix = threshMat,
StimulusMatrix = stimMat)
# Update task performance
X_g <- updateTaskPerformance_Determ(P_sub_g    = P_g,
TaskMat    = X_g,
QuitProb   = quitP,
TimeStep = t,
StimulusMatrix = stimMat)
# Capture current task performance tally
tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)
colnames(tally) <- c("t", colnames(X_g))
tally <- transform(tally, Inactive = n - sum(X_g), n = n, replicate = sim)
taskTally[[t]] <- tally
# Update total task performance profile
X_tot <- X_tot + X_g
# Create time step for correlation
if (t %% corrStep == 0) {
# Get tasks performance in correlation step
X_step <- X_tot - X_prevTot
# Add to ensemble list of task steps
taskStep[[t / corrStep]] <- X_step
# Calculate rank correlation if it is not the first step
if(sum(X_prev) != 0) {
# Normalize
stepNorm <- X_step / rowSums(X_step)
prevNorm <- X_prev / rowSums(X_prev)
# stepNorm <- X_step / corrStep
# prevNorm <- X_prev / corrStep
# Calculate ranks
step_ranks <- calculateTaskRank(TaskStepMat = X_step)
prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)
# Calculate Correlation
rankCorr <- cor(prev_ranks, step_ranks, method = "spearman")
# Put in list
taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)
names(taskCorr)[(t / corrStep) - 1] <- paste0("Gen", t)
}
# Update previous step total matrix
X_prevTot <- X_tot
# Update previous step total matrix
X_prev <- X_step
}
}
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- transform(entropy, n = n, replicate = sim)
# Calculate total task distribution
# totalTaskDist <- X_tot / rowSums(X_tot)
totalTaskDist <- X_tot / gens
totalTaskDist <- transform(totalTaskDist, Inactive = gens - rowSums(X_tot), n = n, replicate = sim)
# Create tasktally table
taskTally <- do.call("rbind", taskTally)
# Create tasktally table
stimMat <- transform(stimMat, n = n, replicate = sim)
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]  <- totalTaskDist
ens_entropy[[sim]]   <- entropy
ens_taskCorr[[sim]]  <- taskCorr
ens_taskTally[[sim]] <- taskTally
ens_taskStep[[sim]]  <- taskStep
ens_stim[[sim]]      <- stimMat
# Print simulation completed
print(paste0("DONE: N = ", n, ", Simulation ", sim))
}
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
colnames(runMean) <- colnames(runs)
return(runMean)
})
runCorrs <- do.call("rbind", runCorrs)
runCorrs <- transform(runCorrs, n = n)
# Add to list of lists
groups_taskDist[[i]]  <- ens_taskDist
groups_taskCorr[[i]]  <- runCorrs
groups_taskStep[[i]]  <- ens_taskStep
groups_taskTally[[i]] <- ens_taskTally
groups_stim[[i]]      <- ens_stim
groups_entropy[[i]]   <- ens_entropy
}
# trim out correlations for group size 1
if(1 %in% Ns) {
groups_taskCorr <- groups_taskCorr[-1]
}
stimMat
plot(stimMat[1:1000,1], type = "l")
P_g
plot(stimMat[1:1000,2], type = "l")
stimMat
P_sub_g = P_g
TaskMat = X_g
TimeStep = t
StimulusMatrix = stimmat
StimulusMatrix = stimMat
look <- groups_taskTally[[1]]
look <- look[[1]]
plot(look$Task1[1:1000])
View(look)
plot(look$Task1[1:1000], type = "l")
plot(look$Task1[1:300], type = "l")
plot(look$Task1[1:200], type = "l")
rm(list = ls())
source("scripts/__Util__MASTER.R")
####################
# Set global variables
####################
# Initial paramters: Free to change
# Base parameters
Ns             <- c(16) #vector of number of individuals to simulate
m              <- 2 #number of tasks
gens           <- 10000 #number of generations to run simulation
corrStep       <- 200 #number of time steps for calculation of correlation
reps           <- 1 #number of replications per simulation (for ensemble)
# Threshold Parameters
ThreshM        <- c(10, 10) #population threshold means
ThreshSD       <- ThreshM * 0 #population threshold standard deviations
InitialStim    <- c(0, 0) #intital vector of stimuli
StimRates      <- c(0.6, 0.6) #vector of stimuli increase rates
threshSlope    <- 30 #exponent parameter for threshold curve shape
alpha          <- m #efficiency of task performance
quitP          <- 0.2 #probability of quitting task once active
####################
# Run simulation multiple times
####################
# Prep meta-lists for collection of group size simulations
groups_taskDist  <- list()
groups_taskCorr  <- list()
groups_taskStep  <- list()
groups_taskTally <- list()
groups_stim      <- list()
groups_entropy   <- list()
# Loop through group sizes
for (i in 1:length(Ns)) {
# Set group size
n <- Ns[i]
# Prep lists for collection of simulation outputs
ens_taskDist  <- list()
ens_taskCorr  <- list()
ens_taskStep  <- list()
ens_taskTally <- list()
ens_entropy   <- list()
ens_stim      <- list()
# Run Simulations
for (sim in 1:reps) {
####################
# Seed structures and intial matrices
####################
# Set initial probability matrix (P_g)
P_g <- initiateProbMatrix(n = n, m = m)
# Seed task (external) stimuli
stimMat <- seedStimuls(InitialSVector = InitialStim,
RateVector = StimRates,
gens = gens)
# Seed internal thresholds
threshMat <- seedThresholds(n = n,
m = m,
ThresholdMeans = ThreshM,
ThresholdSDs = ThreshSD)
# Start task performance
X_g <- matrix(data = rep(0, length(P_g)), ncol = ncol(P_g))
# Create cumulative task performance matrix
X_tot <- X_g
# Prep correlation step matrix
X_prev <- matrix(data = rep(0, n * m), ncol = m)
X_prevTot <- matrix(data = rep(0, n * m), ncol = m)
taskCorr <- list()
taskStep <- list()
taskTally <- list()
####################
# Simulate
####################
# Run simulation
for (t in 1:gens) {
# Update stimuli
for (j in 1:(ncol(stimMat)/2)) {
# update stim
stimMat[t + 1, j] <- globalStimUpdate(stimulus = stimMat[t, j],
delta = stimMat[t, j + m],
alpha = alpha,
Ni = sum(X_g[ , j]),
n = n)
# shift down delta (rate increases)
stimMat[t + 1, j + m] <- stimMat[t, j + m]
}
# Calculate task demand based on global stimuli
P_g <- calcThresholdDetermMat(TimeStep = t + 1, # first row is generation 0
ThresholdMatrix = threshMat,
StimulusMatrix = stimMat)
# Update task performance
# X_g <- updateTaskPerformance_Determ(P_sub_g    = P_g,
#                                     TaskMat    = X_g,
#                                     QuitProb   = quitP,
#                                     TimeStep = t,
#                                     StimulusMatrix = stimMat)
X_g <- updateTaskPerformance(P_sub_g    = P_g,
TaskMat    = X_g,
QuitProb   = quitP)
# Capture current task performance tally
tally <- matrix(c(t, colSums(X_g)), ncol = ncol(X_g) + 1)
colnames(tally) <- c("t", colnames(X_g))
tally <- transform(tally, Inactive = n - sum(X_g), n = n, replicate = sim)
taskTally[[t]] <- tally
# Update total task performance profile
X_tot <- X_tot + X_g
# Create time step for correlation
if (t %% corrStep == 0) {
# Get tasks performance in correlation step
X_step <- X_tot - X_prevTot
# Add to ensemble list of task steps
taskStep[[t / corrStep]] <- X_step
# Calculate rank correlation if it is not the first step
if(sum(X_prev) != 0) {
# Normalize
stepNorm <- X_step / rowSums(X_step)
prevNorm <- X_prev / rowSums(X_prev)
# stepNorm <- X_step / corrStep
# prevNorm <- X_prev / corrStep
# Calculate ranks
step_ranks <- calculateTaskRank(TaskStepMat = X_step)
prev_ranks <- calculateTaskRank(TaskStepMat = X_prev)
# Calculate Correlation
rankCorr <- cor(prev_ranks, step_ranks, method = "spearman")
# Put in list
taskCorr[[(t / corrStep) - 1]] <- diag(rankCorr)
names(taskCorr)[(t / corrStep) - 1] <- paste0("Gen", t)
}
# Update previous step total matrix
X_prevTot <- X_tot
# Update previous step total matrix
X_prev <- X_step
}
}
# Calculate Entropy
entropy <- mutualEntropy(TotalStateMat = X_tot)
entropy <- transform(entropy, n = n, replicate = sim)
# Calculate total task distribution
# totalTaskDist <- X_tot / rowSums(X_tot)
totalTaskDist <- X_tot / gens
totalTaskDist <- transform(totalTaskDist, Inactive = gens - rowSums(X_tot), n = n, replicate = sim)
# Create tasktally table
taskTally <- do.call("rbind", taskTally)
# Create tasktally table
stimMat <- transform(stimMat, n = n, replicate = sim)
# Add total task distributions, entropy values, and graphs to lists
ens_taskDist[[sim]]  <- totalTaskDist
ens_entropy[[sim]]   <- entropy
ens_taskCorr[[sim]]  <- taskCorr
ens_taskTally[[sim]] <- taskTally
ens_taskStep[[sim]]  <- taskStep
ens_stim[[sim]]      <- stimMat
# Print simulation completed
print(paste0("DONE: N = ", n, ", Simulation ", sim))
}
# Calculate mean correlation for each n
runCorrs <- lapply(ens_taskCorr, function(x) {
# Unlist
runs <- do.call("rbind", x)
# Calculate mean
runMean <- matrix(data = rep(NA, m), ncol =  m)
for (column in 1:m) {
runMean[ , column] <- mean(runs[ , column], na.rm = TRUE)
}
colnames(runMean) <- colnames(runs)
return(runMean)
})
runCorrs <- do.call("rbind", runCorrs)
runCorrs <- transform(runCorrs, n = n)
# Add to list of lists
groups_taskDist[[i]]  <- ens_taskDist
groups_taskCorr[[i]]  <- runCorrs
groups_taskStep[[i]]  <- ens_taskStep
groups_taskTally[[i]] <- ens_taskTally
groups_stim[[i]]      <- ens_stim
groups_entropy[[i]]   <- ens_entropy
}
# trim out correlations for group size 1
if(1 %in% Ns) {
groups_taskCorr <- groups_taskCorr[-1]
}
look <- groups_taskTally[[1]][[1]]
plot(look$Task1[1:200], type = "l")
source('~/Documents/Research/Tarnita Lab/Self-Organized DOL/Network Model/NetworkModel-R/scripts/1D_DetThreshModelWithSpecialMetric.R', echo=TRUE)
rm(list = ls())
source("scripts/__Util__MASTER.R")
source("scripts/3A_PrepPlotExperimentData.R")
load("output/__RData/MSrevision_FixedDelta06Sigma01Eta7100reps.Rdata")
# Set variable
filename <- "Fixed_Delta06Sigma01Eta7"
# Palette without single individuals
#palette <- c("#F00924", "#F7A329", "#FDD545", "#027C2C", "#1D10F9", "#4C0E78", "#bdbdbd", "#525252")
# Palette without single individuals
palette <- c("#F00924", "#F7A329", "#FDD545", "#027C2C", "#1D10F9", "#4C0E78", "#bdbdbd", "#525252")
stims_avg <- stims %>%
group_by(n, replicate) %>%
summarise(avgS1 = mean(s1),
avgS2 = mean(s2)) %>%
mutate(Set = paste0(n, "-", replicate),
avgS = (avgS1 + avgS2) / 2)
merged_specstim <- merge(taskCorrTot, stims_avg, by = c("Set", "n"))
palette <- c("#F00924", "#F7A329", "#FDD545", "#027C2C", "#1D10F9", "#4C0E78", "#bdbdbd", "#525252")
qplot(data = merged_specstim, x = TaskMean, y = avgS, col = as.factor(n)) +
theme_classic() +
scale_color_manual(values = palette) +
facet_wrap(~n)
rm(list = ls())
source("scripts/__Util__MASTER.R")
source("scripts/3_PrepPlotExperimentData.R")
load("output/__RData/MSrevision_FixedDelta06_DetThreshDetUpdate100reps.Rdata")
# Set variable
filename <- "Fixed_Delta06Sigma01Eta7"
####################
# Frequency of task not being performed
####################
noTaskPerf <- lapply(groups_taskTally, function(group_size) {
# Loop through replicates within group size
within_groupTaskPerf <- lapply(group_size, function(replicate) {
# Get basics and counts of instances in which there isn't anyone performing task
to_return <- data.frame(n = unique(replicate$n),
replicate = unique(replicate$n),
noTask1 = sum(replicate$Task1 == 0),
noTask2 = sum(replicate$Task2 == 0))
#  Quantify length of no-performance bouts
for (task in c("Task1", "Task2")) {
bout_lengths <- rle(replicate[ , task])
bout_lengths <- as.data.frame(do.call("cbind", bout_lengths))
bout_lengths <- bout_lengths %>%
filter(values == 0)
avg_nonPerformance <- mean(bout_lengths$lengths)
if(task == "Task1") {
to_return$noTask1Length = avg_nonPerformance
}
else {
to_return$noTask2Length = avg_nonPerformance
}
}
# Get averages
to_return <- to_return %>%
mutate(noTaskAvg = (noTask1 + noTask2) / 2,
noTaskLengthAvg = (noTask1Length + noTask2Length) / 2)
# Return
return(to_return)
})
# Bind and return
within_groupTaskPerf <- do.call("rbind", within_groupTaskPerf)
return(within_groupTaskPerf)
})
# Bind
noTaskPerf <- do.call("rbind", noTaskPerf)
# Plot
gg_noTask <- ggplot(data = noTaskPerf, aes(x = n, y = noTask1)) +
geom_point() +
theme_classic() +
scale_x_continuous(breaks = unique(noTaskPerf$n)) +
xlab("Group Size") +
ylab("Instances of No Task 1 Performance")
# ylab("Avg. Length of No Task 1 Performance")
gg_noTask
# Plot
gg_noTask <- ggplot(data = noTaskPerf, aes(x = n, y = noTask1)) +
geom_point() +
theme_classic() +
scale_x_continuous(breaks = unique(noTaskPerf$n)) +
scale_y_continuous(limits = c(0, 6500)) +
xlab("Group Size") +
ylab("Instances of No Task 1 Performance")
# ylab("Avg. Length of No Task 1 Performance")
gg_noTask
# Plot
gg_noTask <- ggplot(data = noTaskPerf, aes(x = n, y = noTask1)) +
geom_point() +
theme_classic() +
scale_x_continuous(breaks = unique(noTaskPerf$n)) +
scale_y_continuous(limits = c(0, 6800)) +
xlab("Group Size") +
ylab("Instances of No Task 1 Performance")
# ylab("Avg. Length of No Task 1 Performance")
gg_noTask
replicate <- groups_taskTally[[7]][[1]]
View(replicate)
plot(replicate$Task1[1:300], type = "l")
look <- groups_stim[[7]][[1]]
plot(look$s1[1:300], type = "l")
plot(replicate$Task1[1:300], type = "l")
line(replicate$Task2[1:300], col = "blue")
line(replicate$Task2[1:300], color = "blue")
lines(replicate$Task2[1:300], color = "blue")
lines(replicate$Task2[1:300], col = "blue")
load("output/__RData/MSrevision_FixedDelta06Sigma01Eta7_Sigma0Eta30100reps.Rdata")
replicate <- groups_taskTally[[7]][[1]]
look <- groups_stim[[7]][[1]]
plot(replicate$Task1[1:300], type = "l")
lines(replicate$Task2[1:300], col = "blue")
lines(replicate$Task2[1:300], col = "red")
plot(replicate$Task1[1:300], type = "l")
lines(replicate$Task2[1:300], col = "red")
plot(replicate$Task1[1:200], type = "l")
lines(replicate$Task2[1:200], col = "red")
load("output/__RData/MSrevision_FixedDelta06_DetThreshDetUpdate100reps.Rdata")
replicate <- groups_taskTally[[7]][[1]]
look <- groups_stim[[7]][[1]]
plot(replicate$Task1[1:200], type = "l")
lines(replicate$Task2[1:200], col = "red")
